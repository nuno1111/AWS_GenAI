{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. RAG를 위한 기본정보와 Prompt Template & 함수 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "from lib.opensearch import getOpenSearchClient\n",
    "aoss_client = getOpenSearchClient()\n",
    "vector_index_name = os.getenv(\"AOSS_VECTOR_INDEX\")\n",
    "\n",
    "from lib.bedrock import get_embedding_output, get_llm_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a helpful assistant to answer the question.\n",
    "Use the following pieces of <CONTEXT> to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Sementic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semantic_rag(user_query):\n",
    "    vector = get_embedding_output(user_query)\n",
    "    vector_query = {\n",
    "      \"query\": {\n",
    "        \"knn\": {\n",
    "          \"content_embeddings\": {\n",
    "            \"vector\": vector,\n",
    "            \"k\": 5\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    response = aoss_client.search(index=vector_index_name, body=vector_query, size=5)\n",
    "    vector_search_results = [result[\"_source\"][\"content\"] for result in response[\"hits\"][\"hits\"]]\n",
    "    \n",
    "    context_data = \"\\n\\n\".join(vector_search_results)\n",
    "    \n",
    "    llm_input = prompt_template.format(context=context_data, question=user_query)\n",
    "    \n",
    "    llm_output = get_llm_output(llm_input)\n",
    "    \n",
    "    return {\"llm_input\": llm_input, \"llm_output\": llm_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제공된 컨텍스트에서 \"요우커지정일\"의 코드는 \"9Y\"입니다. 테이블에 나와 있는 데이터에 따르면 \"Sales\" 카테고리에서 \"CD_VAL\"이 \"요우커지정일\"인 경우 \"CMM_CD\"가 \"9Y\"입니다.\n"
     ]
    }
   ],
   "source": [
    "output = get_semantic_rag(\"요우커지정일의 코드가 어떻게 되나요?\")\n",
    "print(output[\"llm_output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_normalized_result(search_results, add_meta, weight=1.0):\n",
    "    hits = search_results[\"hits\"][\"hits\"]\n",
    "    if len(hits) == 0:\n",
    "        return []\n",
    "    \n",
    "    max_score = float(search_results[\"hits\"][\"max_score\"])\n",
    "    \n",
    "    results = []\n",
    "    for hit in hits:\n",
    "        normalized_score = float(hit[\"_score\"]) / max_score\n",
    "        weight_score = normalized_score if weight == 1.0 else normalized_score * weight\n",
    "        results.append({\n",
    "            \"doc_id\": hit[\"_id\"],\n",
    "            \"score\": weight_score,\n",
    "            \"content\": hit[\"_source\"][\"content\"],\n",
    "            \"meta\": add_meta,\n",
    "            \"metadata\": hit[\"_source\"][\"metadata\"],\n",
    "\n",
    "        })\n",
    "        \n",
    "    return results\n",
    "\n",
    "def get_hybrid_rag(user_query):\n",
    "    result_limit = 5\n",
    "    vec_weight = 0.6\n",
    "    lex_weight = 0.55\n",
    "    threshold = 0.05\n",
    "    \n",
    "    # Get vector search result\n",
    "    vector = get_embedding_output(user_query)\n",
    "    vector_query = {\n",
    "      \"query\": {\n",
    "        \"knn\": {\n",
    "          \"content_embeddings\": {\n",
    "            \"vector\": vector,\n",
    "            \"k\": 5\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    vector_response = aoss_client.search(index=vector_index_name, body=vector_query, size=10)\n",
    "    vector_result = get_normalized_result(vector_response, \"vector\", vec_weight)\n",
    "    \n",
    "    # Get lexical search result\n",
    "    keyword_query = {\"query\": {\"match\": {\"content\": user_query}}}\n",
    "    keyword_response = aoss_client.search(index=vector_index_name, body=keyword_query, size=10)\n",
    "    keyword_result = get_normalized_result(keyword_response, \"lexical\", lex_weight)\n",
    "    \n",
    "    vector_ids = [vec[\"doc_id\"] for vec in vector_result]\n",
    "    for keyword in keyword_result:\n",
    "        if keyword[\"doc_id\"] not in vector_ids:\n",
    "            vector_result.append(keyword)\n",
    "    \n",
    "    items = vector_result\n",
    "    sorted_items = list(filter(lambda val: val[\"score\"] > threshold, items))\n",
    "    \n",
    "    if len(sorted_items) > result_limit:\n",
    "        sorted_items = sorted_items[:result_limit]\n",
    "    \n",
    "    context_data = \"\\n\\n\".join([item[\"content\"] for item in sorted_items])\n",
    "    llm_input = prompt_template.format(context=context_data, question=user_query)\n",
    "    llm_output = get_llm_output(llm_input)\n",
    "    return {\"llm_input\": llm_input, \"llm_output\": llm_output, \"sorted_items\": sorted_items}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return 카테고리에서 CD_VAL 값이 '03'인 행의 내용은 \"수거불필요:택배사분실\"입니다.\n"
     ]
    }
   ],
   "source": [
    "output = get_hybrid_rag(\"Return 카테고리에 03 코드는 어떤 값인가요?\")\n",
    "print(output[\"llm_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '페이지내 이미지 테스트', 'id': '63897603', 'source': 'https://lge-web-project.atlassian.net/wiki/spaces/SD/pages/63897603', 'when': '2024-06-02T01:32:45.382Z'}\n",
      "{'title': '페이지내 이미지 테스트', 'id': '63897603', 'source': 'https://lge-web-project.atlassian.net/wiki/spaces/SD/pages/63897603', 'when': '2024-06-02T01:32:45.382Z'}\n",
      "{'title': '페이지내 이미지 테스트', 'id': '63897603', 'source': 'https://lge-web-project.atlassian.net/wiki/spaces/SD/pages/63897603', 'when': '2024-06-02T01:32:45.382Z'}\n",
      "{'title': '페이지내 이미지 테스트', 'id': '63897603', 'source': 'https://lge-web-project.atlassian.net/wiki/spaces/SD/pages/63897603', 'when': '2024-06-02T01:32:45.382Z'}\n"
     ]
    }
   ],
   "source": [
    "for sorted_item in output[\"sorted_items\"]:\n",
    "    print(sorted_item[\"metadata\"][\"document_metadata\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
